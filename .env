# ===========================================================================
# LLM Provider — pick ONE provider and set BASE_URL + API_KEY
# ===========================================================================

# --- Local Ollama (default for Docker sandbox) ---
BASE_URL=http://ollama:11434/v1
API_KEY=ollama

# --- OpenAI ---
# BASE_URL=https://api.openai.com/v1
# API_KEY=sk-your-openai-api-key-here

# --- Google Gemini ---
# BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# API_KEY=your-gemini-api-key-here

# --- LM Studio (on host) ---
# BASE_URL=http://host.docker.internal:1234/v1
# API_KEY=lm-studio

# ===========================================================================
# Optional: Web Search and Reading Tools
# ===========================================================================
# SERPER_DEV_API_KEY=your-serper-api-key-here
# JINA_API_KEY=your-jina-api-key-here

# ===========================================================================
# Model name — used by workflow YAMLs as ${MODEL_NAME}
# Must match a model pulled in Ollama (see: task ollama:list)
# ===========================================================================
MODEL_NAME=glm-4.7-flash

# ===========================================================================
# Ollama container host port (change if 11434 conflicts with host Ollama)
# ===========================================================================
OLLAMA_HOST_PORT=11435
